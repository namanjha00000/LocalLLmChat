
# ğŸŒŸ Local Open Source LLM Chat App ğŸŒŸ

Welcome to the Local Open Source LLM Chat App! ğŸ—¨ï¸ This app lets you have full control over your conversations while chatting with powerful open-source LLMsâ€”without worrying about your data or privacy. Itâ€™s everything you need for a seamless, offline chat experience! ğŸš€

## ğŸ”¥ Features

- **ğŸ’¬ Chat Locally on Open Source LLMs**: Converse with cutting-edge language models directly on your machine. No internet required!
- **ğŸ”’ Full Control Over Your Data & Privacy**: Say goodbye to the data privacy concerns of online services. Your data stays local, safe, and secure. 
- **âš¡ Always Active**: Even if the internet goes down, your chat never stops. Always available, anytime, anywhere!
- **ğŸ—¨ï¸ Multiple Chats**: Start as many conversations as you like and easily switch between them.
- **ğŸŒ Chat with Different Open Source LLMs**: Opt-in to connect with various open-source models. Flexibility at its finest!
- **ğŸ¨ Friendly & Intuitive UI**: Enjoy a sleek, user-friendly interface designed to make chatting a breeze.
- **ğŸ“„ Document Upload (Coming Soon)**: Upload documents and discuss them with your LLM.
- **ğŸ¤– RAG-based Chat (Coming Soon)**: Power up your chats with Retrieval-Augmented Generation for even more insightful and advanced conversations.

## ğŸ› ï¸ Setup Instructions

### Prerequisites

Before you dive in, make sure you have Python 3.x installed. Then, clone this repository by using the command:

```bash
git clone https://github.com/namanjha00000/LocalLLmChat.git
cd LocalLLmChat
```
Now if you are a windows user simply run the run.bat script using the following command in the project folder command prompt:
```bash
run.bat
```

For linux/Mac users, run the following command:
```bash
chmod +x run.sh
./run.sh
```

### Enjoy chatting with your own LLM

